import requests
import sys
from threading import Thread, Lock
from queue import Queue
import argparse

# çº¿ç¨‹é”ï¼Œç”¨äºå®‰å…¨è¾“å‡º
print_lock = Lock()

def test_get_request(full_url, timeout=10):
    """
    æµ‹è¯•URLçš„GETè¯·æ±‚ï¼Œæ£€æŸ¥çŠ¶æ€ç å’Œå“åº”å†…å®¹
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*'
        }
        
        response = requests.get(
            full_url, 
            headers=headers, 
            timeout=timeout,
            verify=False
        )
        
        # æ£€æŸ¥çŠ¶æ€ç æ˜¯å¦ä¸º200ä¸”ä¸åŒ…å«"errors"
        if response.status_code == 200 and "errors" not in response.text.lower():
            with print_lock:
                print(f"ğŸ¯ [SUCCESS] {full_url}")
                print(f"   çŠ¶æ€ç : {response.status_code}")
                print(f"   å“åº”é•¿åº¦: {len(response.text)}")
                print("-" * 60)
            
            # ä¿å­˜æˆåŠŸçš„URL
            save_success_url(full_url, response.text)
            return True
        else:
            with print_lock:
                if response.status_code != 200:
                    print(f"âŒ [FAIL] {full_url} - çŠ¶æ€ç : {response.status_code}")
                else:
                    print(f"âŒ [FILTERED] {full_url} - åŒ…å«'errors'")
            return False
                
    except requests.exceptions.RequestException as e:
        with print_lock:
            print(f"âš ï¸ [ERROR] {full_url} - {str(e)}")
        return False
    except Exception as e:
        with print_lock:
            print(f"ğŸ’¥ [UNKNOWN ERROR] {full_url} - {str(e)}")
        return False

def save_success_url(url, response_text):
    """ä¿å­˜æˆåŠŸçš„URLåˆ°æ–‡ä»¶"""
    try:
        with open("success_urls.txt", "a", encoding="utf-8") as f:
            f.write(f"URL: {url}\n")
            #f.write(f"å“åº”å†…å®¹:\n{response_text}\n")
            #f.write("=" * 80 + "\n\n")
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶é”™è¯¯: {str(e)}")

def worker(url_queue, timeout):
    """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
    while not url_queue.empty():
        url = url_queue.get()
        test_get_request(url, timeout)
        url_queue.task_done()

def main():
    parser = argparse.ArgumentParser(description='å¤šçº¿ç¨‹GETè¯·æ±‚æµ‹è¯• - è¿‡æ»¤åŒ…å«"errors"çš„å“åº”')
    parser.add_argument('file', help='åŒ…å«å®Œæ•´URLåˆ—è¡¨çš„txtæ–‡ä»¶')
    parser.add_argument('-t', '--threads', type=int, default=10, 
                       help='çº¿ç¨‹æ•°é‡ (é»˜è®¤: 10)')
    parser.add_argument('-to', '--timeout', type=int, default=8,
                       help='è¯·æ±‚è¶…æ—¶æ—¶é—´ (é»˜è®¤: 8ç§’)')
    
    args = parser.parse_args()
    
    try:
        # è¯»å–URLæ–‡ä»¶
        with open(args.file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        
        if not urls:
            print("æ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰æœ‰æ•ˆçš„URL")
            return
        
        print(f"ğŸ“ å…±è¯»å–åˆ° {len(urls)} ä¸ªURL")
        print(f"ğŸ”§ çº¿ç¨‹æ•°é‡: {args.threads}")
        print(f"â±ï¸ è¶…æ—¶æ—¶é—´: {args.timeout}ç§’")
        print("ğŸ¯ æˆåŠŸæ¡ä»¶: çŠ¶æ€ç 200ä¸”ä¸åŒ…å«'errors'")
        print("ğŸš€ å¼€å§‹å¤šçº¿ç¨‹GETè¯·æ±‚æµ‹è¯•...")
        print("=" * 60)
        
        # æ¸…ç©ºä¹‹å‰çš„æˆåŠŸæ–‡ä»¶
        open("success_urls.txt", "w", encoding="utf-8").close()
        
        # åˆ›å»ºé˜Ÿåˆ—
        url_queue = Queue()
        for url in urls:
            url_queue.put(url)
        
        # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
        thread_count = min(args.threads, len(urls))
        threads = []
        
        print(f"å¯åŠ¨ {thread_count} ä¸ªçº¿ç¨‹...")
        
        for i in range(thread_count):
            thread = Thread(target=worker, args=(url_queue, args.timeout))
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        url_queue.join()
        
        print("=" * 60)
        print("âœ… æµ‹è¯•å®Œæˆï¼æˆåŠŸçš„URLå·²ä¿å­˜åˆ°: success_urls.txt")
        
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ {args.file} ä¸å­˜åœ¨")
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")

if __name__ == "__main__":
    # ç¦ç”¨SSLè­¦å‘Š
    requests.packages.urllib3.disable_warnings()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ç‰ˆæœ¬
    if len(sys.argv) > 1:
        main()
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python get_test.py urls.txt")
        print("python get_test.py urls.txt -t 20")
        print("python get_test.py urls.txt -t 15 -to 5")
        print("\nè¯´æ˜:")
        print("- URLæ–‡ä»¶åº”åŒ…å«å®Œæ•´çš„GETè¯·æ±‚åœ°å€")
        print("- åªä¿å­˜çŠ¶æ€ç 200ä¸”ä¸åŒ…å«'errors'çš„å“åº”")
        print("- ç»“æœä¿å­˜åˆ° success_urls.txt")